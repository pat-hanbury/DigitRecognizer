{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-dd0d7c3cd3f2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-dd0d7c3cd3f2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    RESOURCE: https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "RESOURCE: https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, Image\n",
    "\n",
    "path = \"data/\"\n",
    "train_path = path + 'train.csv'\n",
    "test_path = path + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/gitProjects/DigitRecognizer/data/sample_submission.csv\n",
      "/home/paperspace/gitProjects/DigitRecognizer/data/train.csv\n",
      "/home/paperspace/gitProjects/DigitRecognizer/data/test.csv\n"
     ]
    }
   ],
   "source": [
    "for x in Path.iterdir(PATH/'data'):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import CSV into pandas dataframe and convert to np array\n",
    "test_data = pd.read_csv(PATH/'data/test.csv')\n",
    "train_data = pd.read_csv(PATH/'data/train.csv')\n",
    "test_np_data = test_data.values;\n",
    "train_np_data = train_data.values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn a row of our data into a square image to be used by matplotlib\n",
    "def getImg(img_in, h, w):\n",
    "    #input is a 1D array (single row in our 2D np array representation of csv file)\n",
    "    img_out = np.zeros(shape=(h,w))\n",
    "    row = 0\n",
    "    for x in range(1,h*w): #sine index 1 is the label\n",
    "        col = x%w\n",
    "        img_out[row][col] = img_in[x]\n",
    "        if col == w-1:\n",
    "            row = row + 1\n",
    "    return img_out\n",
    "            \n",
    "def show_npimg(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def show_im(img, h=28,w=28):\n",
    "    temp = getImg(img, h, w)\n",
    "    show_npimg(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##show a single image\n",
    "show_im(train_np_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ea9556ba2fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_np_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mval_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "#split train set for validation\n",
    "val_split = 0.2\n",
    "num_img = len(train_np_data)\n",
    "idx = [range(0,num_img-1)]\n",
    "val_idx = np.random.choice(idx,int(num_img*val_split),replace=True)\n",
    "sorted(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(val_split*num_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Prepare data in Numpy\n",
    "def get3D(data1D, h=28, w=28):\n",
    "     #input is a 1D array (single row in our 2D np array representation of csv file)\n",
    "     #output is a 3D tensor (channel,height,width)\n",
    "    np_out = np.zeros(shape=(1,h,w))\n",
    "    row = 0\n",
    "    for x in range(1,h*w): #sine index 1 is the label\n",
    "        col = x%w\n",
    "        np_out[0][row][col] = data1D[x]\n",
    "        if col == w-1:\n",
    "            row = row + 1\n",
    "    return np_out\n",
    "\n",
    "def get4D(data2D, h=28, w=28):\n",
    "    #input is 2D array (image number x pixel values)\n",
    "    #output is 4D tensor (image number x channel x h x w)\n",
    "    num_img = len(data2D)\n",
    "    np_out = np.zeros(shape=(num_img, 1, h, w))\n",
    "    for i in range(num_img):\n",
    "        np_out[i] = get3D(data2D[i])\n",
    "    return torch.from_numpy(np_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create Dataset and Dataloader\n",
    "#features needs to be 4D, where each index 0 is the image, index 1 is the channel, and 3/4 are h/w\n",
    "#targets needs to be 1D, where each index represents an image\n",
    "train_features = get4D(train_np_data)\n",
    "train_targets = torch.from_numpy(train_np_data[:,0])\n",
    "train_targets.size() , train_features.size()\n",
    "trn_data = utils.TensorDataset(train_features, train_targets)\n",
    "train_loader = utils.DataLoader(trn_data,batch_size=64,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BnLayer class from FastAI -- \n",
    "class BnLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni,nf,kernel_size=kernel_size, stride=stride,\n",
    "                            bias=False, padding=1)\n",
    "        self.add = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.mult = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        x = F.relu(self.conv(inp))\n",
    "        #don't understand how they are calculating the means\n",
    "        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            self.means = x_chan.mean(1)[:,None,None]\n",
    "            self.stds = x_chan.std(1)[:None,None]\n",
    "        return (x-self.means)/self.stds * self.mult + self.add\n",
    "    \n",
    "#Resnet Layer from FastAI -- \n",
    "class ResnetLayer(BnLayer):\n",
    "    def forward(self,inp): return inp + super().forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet Model from FastAi --\n",
    "class Resnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes in layers (list)  and c (int, number of classes) as input\n",
    "    \"\"\"\n",
    "    def __init__(self,layers,c):\n",
    "        set_trace()\n",
    "        super(Resnet,self).__init__()\n",
    "        #initial large Conv Layer (applied once)\n",
    "        self.conv1 = nn.Conv2d(3,10,kernel_size=5, stride=1, padding=2)\n",
    "        self.layers1 = nn.ModuleList([BnLayer(layers[i],layers[i+1])\n",
    "            for i in range(len(layers)-1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1],layers[i+1],\n",
    "            stride=1) for i in range(len(layers)-1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1],layers[i+1],\n",
    "            stride=1) for i in range(len(layers)-1)])\n",
    "        self.out = nn.Linear(layers[-1],c)\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        x = self.conv1(inp)\n",
    "        for l1,l2,l3 in zip(self.layers1, self.layers2, self.layers3):\n",
    "            x = l3(l2(l1(x)))\n",
    "        x = F.adaptive_max_pool2d(x,1)\n",
    "        #don't get this\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?   :   True\n"
     ]
    }
   ],
   "source": [
    "print('Is GPU available?   :   ' + str(torch.cuda.is_available()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-15-7b3c1b2b8dd0>\u001b[0m(8)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      6 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 8 \u001b[0;31m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      9 \u001b[0;31m        \u001b[0;31m#initial large Conv Layer (applied once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "layers = [10, 20, 40]\n",
    "num_classes = 10\n",
    "model=Resnet(layers, num_classes)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "wd=0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = Adam(model.parameters(),lr=lr, weight_decay=wd )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "###KEY FUNCTIONS FOR TRAINING###\n",
    "\n",
    "#FOO: change this to take in a list of lrs. annd adjust LR\n",
    "#function to get a learning rate while training (using discrimative learning rates)\n",
    "def get_lr_rate(epoch, num_epochs, init_lr=0.001, lr_div=10):\n",
    "    lr = init_lr\n",
    "    if epoch/num_epochs > 0.6:\n",
    "        lr = lr/(lr_div*lr_div)\n",
    "    elif epoch/num_epochs > 0.3:\n",
    "        lr = lr/lr_div\n",
    "    return lr\n",
    "\n",
    "#function for saving models\n",
    "def save_models(m,idx,fn='digit_rec_model_'):\n",
    "    torch.save(m.state_dict(), f\"{fn}{idx}\")\n",
    "    print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Training Function###\n",
    "#def get_trn_loss(m,activations,labels):\n",
    "      \n",
    "\n",
    "#def get_val_acc(model,train):\n",
    "    \n",
    "def get_loss(act,targets):\n",
    "    #computes loss for a given batch of activations (act) and target values (targets)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion(act,targets)\n",
    "    \n",
    "def predict(activs):\n",
    "    _,predictions = torch.max(activs.data,1)\n",
    "    return predictions\n",
    "\n",
    "def train(m,num_epochs,init_lr=0.001):\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for param_group in opt.param_groups:\n",
    "        param_group[\"lr\"] = init_lr\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() #set model to training mode\n",
    "        acc = 0.0\n",
    "        loss = 0.0\n",
    "        for i, (images,gt) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images.cuda()\n",
    "                gt.cuda()\n",
    "            opt.zero_grad() #zero all gradients\n",
    "            set_trace()\n",
    "            outputs=model(images) #get output activations\n",
    "            loss = get_loss(outputs,gt)\n",
    "            loss.backward() #backprop loss\n",
    "            opt.step() #change parameters\n",
    "            \n",
    "            tot_loss += loss.data[0]\n",
    "            tot_acc += acc\n",
    "            \n",
    "        lr = get_lr_rate(epoch, num_epochs, init_lr)\n",
    "        for param_group in opt.param_groups: \n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        acc = get_val_acc(m)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            save_models(model,epoch)\n",
    "        #show metrics for each epoch\n",
    "        print(\"Epoch: {epoch}   TrnLoss: {loss}   ValAcc: {acc}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-87-10515f4f4467>\u001b[0m(32)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     30 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#zero all gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m            \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 32 \u001b[0;31m            \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get output activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     34 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#backprop loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 0 is not a Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-ccb4bf6c9913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-10515f4f4467>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(m, num_epochs, init_lr)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#zero all gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get output activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#backprop loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7b3c1b2b8dd0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 282\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 0 is not a Variable"
     ]
    }
   ],
   "source": [
    "train(model,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.autograd.Variable(torch.randn((3,5)))\n",
    "tgt = torch.autograd.Variable(torch.LongTensor(3).random_(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = loss(input,tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss(\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9108232259750366"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
